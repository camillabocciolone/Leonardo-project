{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/camillabocciolone/Leonardo-project/blob/main/ll2_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vF4daPF7vTI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdN-XgFWKEnc"
   },
   "source": [
    "# Foundation Models:\n",
    "\n",
    "- L’EEG registra l’attività elettrica cerebrale e ha grande valore clinico e scientifico.\n",
    "- L’analisi tradizionale si basa sull’ispezione visiva di esperti o su feature ingegnerizzate.\n",
    "- I modelli deep learning supervisionati (EEG-DL) hanno migliorato le performance su specifici compiti, ma soffrono di overfitting, mancanza di interpretabilità e scarsa trasferibilità tra dataset e soggetti.\n",
    "- I Foundation Models (FMs), pre-addestrati su grandi quantità di dati non etichettati tramite Self-Supervised Learning (SSL), offrono una soluzione potenziale, imparando rappresentazioni generalizzabili riutilizzabili su vari task con pochi dati etichettati.\n",
    "\n",
    "\n",
    "- **EEG Foundation Models: A Critical Review of Current Progress and Future Directions**. Comparison:\n",
    "\n",
    "    BrainBERT\n",
    "\n",
    "    Neuro-GPT\n",
    "\n",
    "    Brant\n",
    "\n",
    "    BIOT\n",
    "\n",
    "    *EEGFormer*\n",
    "\n",
    "    LaBraM\n",
    "    \n",
    "    *Mentality*\n",
    "    \n",
    "    NeuroLM\n",
    "    \n",
    "    FoME\n",
    "    \n",
    "    *BrainWave*\n",
    "   \n",
    "   https://arxiv.org/pdf/2507.11783\n",
    "\n",
    "   contenuto:\n",
    "   \n",
    "   Gli autori scompongono un EEG-FM in alcuni pilastri principali:\n",
    "   \n",
    "   1) Rappresentazione dei dati di input\n",
    "\n",
    "        L’EEG può essere rappresentato come segnale nel tempo, spettro di potenza o rappresentazione tempo-frequenza.\n",
    "        La scelta dell’input influenza la qualità delle rappresentazioni apprese.\n",
    "  2) Architettura del modello\n",
    "\n",
    "      Quasi tutti gli EEG-FMs si basano su transformer, a volte combinati con strati convoluzionali per catturare pattern locali.\n",
    "      Alcuni includono tokenizzatori o codebook (tipo VQ-VAE) per quantizzare le rappresentazioni.\n",
    "  3) Self-supervised learning (SSL)\n",
    "\n",
    "      I task SSL comuni sono: ricostruzione di sequenze mascherate, modellazione autoregressiva o contrastive learning.\n",
    "      Lo scopo è imparare relazioni intrinseche nei dati senza etichette.\n",
    "  4) Transfer learning e adattamento\n",
    "  \n",
    "      Dopo il pretraining, il backbone viene riutilizzato e adattato (fine-tuning o linear probing) per task specifici.\n",
    "  5) Scala di dati e modello\n",
    "\n",
    "      Maggiori quantità di dati e parametri migliorano la qualità delle rappresentazioni, ma l’effetto della scalabilità nell’EEG non è ancora ben dimostrato.\n",
    "\n",
    "Pipeline di un EEG Foundation Model\n",
    "\n",
    "1️⃣ Raccolta e preparazione dei dati\n",
    "Input: segnali EEG multi-canale (scalp o intracraniali).\n",
    "Preprocessing minimo:\n",
    "- filtraggio passa-banda (0.5–60 Hz),\n",
    "- notch a 50/60 Hz,\n",
    "- risampling (200–250 Hz),\n",
    "- segmentazione in patch da 1–10 s.\n",
    "\n",
    "Nessuna etichetta: i dati sono grezzi e non annotati.\n",
    "\n",
    "Rappresentazione:\n",
    "- nel tempo (raw time series) → più comune;\n",
    "- oppure spettro / tempo-frequenza (STFT, wavelet).\n",
    "\n",
    "2️⃣ Definizione della rappresentazione di input\n",
    "\n",
    "- Ogni segmento EEG X∈R può essere diviso in token (piccoli blocchi temporali);\n",
    "- arricchito con positional embeddings (tempo + canale) per dare al modello nozioni di ordine e topografia.\n",
    "\n",
    "3️⃣ Architettura del modello\n",
    "\n",
    "Backbone (feature extractor): quasi sempre un Transformer, a volte preceduto da strati convoluzionali per pattern locali.\n",
    "\n",
    "Tokenizzatore opzionale: (LaBraM, NeuroLM, EEGFormer) → usa un codebook discreto tipo VQ-VAE per quantizzare le rappresentazioni.\n",
    "\n",
    "Decoder: ricostruisce o predice parti mascherate del segnale.\n",
    "\n",
    "Struttura tipica:\n",
    "Input EEG  →  Tokenization  →  Positional Embeddings\n",
    "           →  Encoder (Transformer/CNN)\n",
    "           →  Decoder (per SSL task)\n",
    "\n",
    "\n",
    "4️⃣ Pretraining (Self-Supervised Learning)\n",
    "\n",
    "Scopo: imparare rappresentazioni EEG generali senza etichette.\n",
    "\n",
    "Task SSL principali:\n",
    "Tipo di SSL\tEsempio\tObiettivo\n",
    "- Masked reconstruction\tBrainBERT, LaBraM\tRicostruire patch nascoste del segnale\n",
    "- Contrastive learning\tBIOT\tAvvicinare rappresentazioni di viste simili del segnale\n",
    "- Autoregressivo\tNeuroLM\tPredire il prossimo token a partire dai precedenti\n",
    "\n",
    "👉 Il modello impara così una rappresentazione latente compatta e robusta del segnale.\n",
    "\n",
    "5️⃣ Salvataggio del backbone pre-addestrato\n",
    "\n",
    "Dopo il pretraining, il backbone (encoder) diventa un feature extractor generalista:\n",
    "\n",
    "rappresenta EEG di soggetti, sessioni o dispositivi diversi in uno spazio latente comune.\n",
    "\n",
    "6️⃣ Adattamento a nuovi task (Transfer learning)\n",
    "\n",
    "Con piccoli dataset etichettati:\n",
    "\n",
    "Tipo di adattamento\tDescrizione\n",
    "- Linear probing\tBackbone congelato, si allena solo una testa lineare di classificazione.\n",
    "- Fine-tuning\tSi aggiornano anche i pesi del backbone (tutto o in parte).\n",
    "- Few-shot / Zero-shot\tSi usa il backbone quasi “as-is” per nuovi task, sfruttando la similarità nello spazio latente.\n",
    "\n",
    "8️⃣ Miglioramenti futuri suggeriti\n",
    "- Usare dataset più vari (scalp + iEEG + sleep + BCI).\n",
    "- Estendere la lunghezza del contesto temporale (> 90 s).\n",
    "- Aggiungere attenzione spaziale tra canali.\n",
    "- Fare valutazioni standardizzate e zero-shot.\n",
    "- Integrare federated learning e multi-modalità (EEG + - testo, ECG, video).\n",
    "- Studiare interpretabilità e affidabilità clinica.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives**. Comparison:\n",
    "    Brant,\n",
    "    \n",
    "    Brant-2\n",
    "    \n",
    "    LaBraM\n",
    "    \n",
    "    NeuroGPT\n",
    "    \n",
    "    BIOT\n",
    "    \n",
    "    EEGPT1\n",
    "    \n",
    "    BrainBERT\n",
    "    \n",
    "    FoME\n",
    "    \n",
    "    EEGPT2\n",
    "    \n",
    "    MBrain\n",
    "    \n",
    "    NeuroLM\n",
    "    \n",
    "    CBraMod\n",
    "    \n",
    "    EEGFormer\n",
    "    \n",
    "    ALFEE\n",
    "\n",
    "    https://arxiv.org/pdf/2504.20069\n",
    "\n",
    "- **EEGMamba: An EEG foundation model with Mamba**\n",
    "Propone un modello foundation per EEG, denominato EEGMamba, apprendendo rappresentazioni generiche dell’EEG che possono essere usate per vari compiti downstream.\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0893608025006963?utm_source=chatgpt.com\n",
    "\n",
    "- **Graph-Enhanced EEG Foundation Model (GEFM)**\n",
    "Combina modelli transformer (o autoencoder mascherati) con componenti di Graph Neural Network (GNN) per catturare sia la dinamica temporale che le relazioni tra canali EEG. Questo approccio ibrido mira a modellare i segnali EEG come grafi con nodi = canali. https://arxiv.org/html/2411.19507v1?utm_source=chatgpt.com\n",
    "\n",
    "# Graph Neural Networks / Machine Learning\n",
    "**se  >= 16 canali può avere senso usare graph neural network mentre con meno meglio graph machine learning**\n",
    "\n",
    "- **A review of Graph Neural Networks for Electroencephalography data analysis** Although EEG sensor do not provide actual brain localizations of the activity sources, they allow to study brain functional connectivity. In this paper we review current application of a specific family of computational methods, the Graph Neural Networks (GNN) to the analysis of EEG data. GNNs appear to be well suited to EEG data modeling as they deal with signals whose domain is defined by a graph instead of a regular lattice in Euclidean space.\n",
    "https://www.sciencedirect.com/science/article/pii/S092523122301024X\n",
    "\n",
    "- **Revealing brain connectivity: graph embeddings for EEG representation learning and comparative analysis of structural and functional connectivity**: classificazione EEG per compiti di motor imagery. In particolare lo studio propone due modelli GCN per la classificazione EEG di motor imagery, uno basato su connessioni strutturali (Adj-CNNM) e l’altro su connessioni funzionali (PLV-CNNM).\n",
    "Il secondo risulta più accurato e, oltre a migliorare la classificazione, aiuta a comprendere meglio la connettività cerebrale durante i compiti motori.\n",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC10804888/?utm_source=chatgpt.com\n",
    "\n",
    "- **Graph-Based Learning for EEG Workload Classification** Vogliono stimare il Mental Workload (MWL) da EEG che generalizzi a soggetti mai visti, usando reti su grafi (GNN) invece di feature fatte a mano.\n",
    "\n",
    "Dati e pipeline (uguale per entrambi i dataset)\n",
    "  * Mantis (privato): 100 soggetti, 32 canali, 500 Hz (→ 250 Hz), task N-back (0–5).\n",
    "  * STEW (pubblico): 48 soggetti, 14 canali, 128 Hz; baseline vs SIMKAP.\n",
    "  * Preproc: resampling 250 Hz, notch 50 Hz, rimozione canali “cattivi”, common average reference, band-pass 1–30 Hz, epoche da 1 s (overlap 0.3 Mantis / 0.5 STEW), stack K=30 (Mantis), K=15 (STEW).\n",
    "  * Valutazione: LOSOCV = ogni giro lasci fuori un soggetto per il test. È lo standard corretto per verificare generalizzazione cross-subject (niente calibrazione del nuovo soggetto).\n",
    "\n",
    "  Modelli confrontati\n",
    "  * GGN (da Li et al., 2022): genera il grafo di connettività, attentive graph conv + CNN + classifier FC.\n",
    "  * GNN (Tang et al., 2021): GNN self-supervised che usa geometria degli elettrodi o connettività dinamica.\n",
    "  * Baseline: CNN (tipo SeizureNet) e Transformer (implementazioni da letteratura MWL).\n",
    "\n",
    "  https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://2025.ccneuro.org/abstract_pdf/Bocini_2025_Graph-Based_Learning_EEG_Workload_Classification_Eliminating.pdf&ved=2ahUKEwiPxoXFnY2QAxXLUaQEHQbQAI4QFnoECBgQAQ&usg=AOvVaw3vsIgV2WBC7BpDVNskjehe\n",
    "\n",
    "# Functional Data Analysis\n",
    "- **Classification of EEG signals: An interpretable approach using functional data analysis**:\n",
    "Electroencephalography (EEG) is a noninvasive method to record electrical activity of the brain. The EEG data is continuous flow of voltages, in this paper, we consider them as functional data, and propose a three-stage algorithm based on functional data analysis, with the advantage of interpretability. Specifically, the time and frequency information are extracted by wavelet transform in the first stage. Then, functional testing is utilized to select EEG channels and frequencies that show significant differences for different human behaviors. In the third stage, we propose to use penalized multiple functional logistic regression to interpretably classify human behaviors. With simulation and a scalp EEG data as validation set, we show that the proposed three-stage algorithm provides an interpretable classification of the scalp EEG signals\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0165027022001364\n",
    "\n",
    "- Hasenstab et al., 2017 (Biometrics) — M(ulti-) D-FPCA su ERP multicanale (tempo×trial×elettrodo) per estrarre componenti funzionali multivariati su cui poi si possono addestrare classificatori; lavoro “cardine” per la fase FPCA/MFPCA su EEG https://pubmed.ncbi.nlm.nih.gov/28072468/\n",
    "\n",
    "- Paper: Nie, Wang, Liu, Cao, “Supervised Functional Principal Component Analysis”. Applicazione reale su EEG (alcolisti vs controlli).\n",
    "\n",
    "  Nel paper si chiarisce esplicitamente che l’approccio convenzionale consiste nel rappresentare i segnali come funzioni, calcolare le componenti principali funzionali (FPC) e usare i relativi punteggi (score) come variabili per predire l’esito. Questo è scritto già nell’introduzione: prima si fa FPCA sul predittore funzionale e poi si impiegano i primi score FPC per stimare il modello di risposta. Questo corrisponde esattamente alla pipeline che vuoi implementare.\n",
    "\n",
    "  Il modello usato è un modello lineare funzionale generalizzato, con link logistico quando la risposta è binaria; gli autori richiamano anche l’estensione ai casi multinomiali. Quindi la stessa struttura FPCA→score si presta sia a classificazione binaria sia, per estensione, a problemi con più classi.\n",
    "\n",
    "  Nella sezione applicativa su EEG gli autori descrivono in modo operativo la pipeline: a) costruiscono curve EEG per sensore e soggetto; b) stimano le prime p FPC per ciascun sensore con la loro variante supervisionata; c) passano alla classificazione stimando una logistica funzionale multivariata, che viene riscritta direttamente in funzione degli score FPC per sensore (somma degli score per sensore moltiplicati da coefficienti da stimare); d) applicano una penalizzazione L1 sui coefficienti e selezionano i parametri tramite cross-validation solo sul training, quindi valutano l’errore di classificazione sul test. È tutto specificato, inclusa l’equazione della logistica in termini di score e il protocollo di selezione dei parametri.\n",
    "\n",
    "  Infine riportano una tabella con gli errori di classificazione medi e deviazioni standard al variare del numero di FPC: la versione supervisionata della FPCA riduce l’errore rispetto alla FPCA “standard”, confermando la bontà della pipeline “base → FPC → score → classificatore”. Questo fornisce anche numeri di riferimento per la tua implementazione.\n",
    "\n",
    "  In sintesi, lo stesso articolo ti dà: (i) giustificazione della pipeline FPCA→score→classificatore, (ii) formulazione del modello logistico funzionale (anche estendibile a più classi), (iii) applicazione concreta su EEG con passaggi pratici (stima FPC per sensore, logistica sugli score, CV sul training, valutazione sul test) e (iv) risultati comparativi che mostrano il vantaggio dell’approccio.\n",
    "\n",
    "  https://link.springer.com/article/10.1007/s11222-017-9758-2\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYmSvjcBvAGB"
   },
   "source": [
    "proviamo il foundation model CBraMod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fjvwvMBY0ifx",
    "outputId": "cc7e1280-bd9d-427a-adf5-ca3968bfbbd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing repo requirements...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CBraMod 🚀 Fine-tuning su EEG (Colab-ready)\n",
    "# ============================================================\n",
    "# ✅ Funziona con dati fittizi (smoke test)\n",
    "# ✅ Poi switcha facilmente ai tuoi dati reali\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# --- 0) Colab/Env Setup ---\n",
    "import os, sys, subprocess, math, random, time\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "# (Opzionale) Monta Google Drive se vuoi salvare i risultati lì\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    MOUNT_DRIVE = False  # cambia a True se vuoi montare Drive\n",
    "    if MOUNT_DRIVE:\n",
    "        drive.mount('/content/drive')\n",
    "\n",
    "WORKDIR = \"/content\"\n",
    "os.chdir(WORKDIR)\n",
    "\n",
    "# --- 1) Dipendenze & Repo ---\n",
    "# Colab ha già torch; installiamo solo ciò che serve\n",
    "!pip -q install einops huggingface_hub scikit-learn tqdm\n",
    "\n",
    "# Clona la repo CBraMod se non presente\n",
    "REPO_DIR = Path(\"CBraMod\")\n",
    "if not REPO_DIR.exists():\n",
    "    !git clone -q https://github.com/wjq-learning/CBraMod.git\n",
    "os.chdir(\"CBraMod\")\n",
    "\n",
    "# Prova a installare requirements del repo (se fallisce, continuiamo lo stesso)\n",
    "req_path = Path(\"requirements.txt\")\n",
    "if req_path.exists():\n",
    "    try:\n",
    "        print(\"Installing repo requirements...\")\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", \"requirements.txt\", \"-q\"], check=False)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: requirements install failed, continue anyway:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284,
     "referenced_widgets": [
      "f9c700fc8060490295d68341b7cc4537",
      "d15afd9a3aa94023a2e9614f9ecbea5e",
      "825a828043f946299c84873bc91f8096",
      "88183ed00d294cf496dd958ff185205c",
      "24d44a9351a14cb4a8361333a5d2461c",
      "05e8e2c53eb64e54aa48182bd885ca15",
      "a00f1d33319a4627aa03ca321bedb508",
      "6bce073a54ce4ddcba5875f322e52440",
      "e8f2ab75479d4d23a476de9f3dcc113d",
      "20a6a37cdf9f4f7e986a2836e8a0283a",
      "ecd6cc5608e744f99a05cb001e1268e6",
      "91ad812dcf5c4965b4bf83e2afb1d49f",
      "da665ff94a2441118ed91bf37a781628",
      "c770ac225fb6442085aae245539c1bfe",
      "58b022f2547e4a4484406ffb6dd444d8",
      "f39ab7714a284a30812ad5947e3ce1d4",
      "369052ba9b9c47a88ba2999f2cdeb44c",
      "a3aec78db67f45df90ce010b4934d28e",
      "a10105fe9e814a3f8247b9ab263eb755",
      "3b6425ef1e4044458e9a99fcecc188af",
      "7846e7c59e334264aabcc981cd937acc",
      "2157ca25b2394fde9b1c1c0a5a466432",
      "9640b8dab6fc4699b1221fe312bc2e57",
      "2de09f5a474449338ba0c9220921e24d",
      "8401ca600d2f407fa26fb88328e866e3",
      "76e3009e26be47b18fa62e603dc742fd",
      "d29d9cf445ac42af80610c545068bca4",
      "c99eeb681ce74324aeeff5229789f980",
      "1ec3b2a6d5574478bfcd287df8c38eda",
      "e5c8544adaf44f9ba1ced5bf6d9f72e9",
      "64928a7cde224193a9809cc11dfde210",
      "0b3ebbdfca164bd7873f23f2e4477710",
      "0ffb292584bc4d0baa3c5e4aebc148c8",
      "2672087aab76473aad4ccae7f24439fd",
      "3ce358f0bff44901ab59da1ae7150bb7",
      "ea3f55e2f19e466a9e20eb63a6bb1326",
      "a39f7571435f461785d4ebdff4f26163",
      "618ea361411349deaad24bed5545aea0",
      "d9e7b3a563934bb69a4d40b382b9436c",
      "4232ec3ea15f461eb6e8fc2d84b509be",
      "70f4c0bfce5c44adb3712885e103e13a",
      "b53317fd2d6b411fafaf77fdb5a3acbf",
      "0214b826afba449d918f96a58839e697",
      "dc115d56fbaa4a059152c3e4b9a85439"
     ]
    },
    "id": "p9bCSeHm057A",
    "outputId": "0c2b1613-f03e-4c2d-fb3b-fe0f3357b857"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/75.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pretrained_weights.pth:   0%|          | 0.00/19.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       ".gitattributes: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint copiato in: pretrained_weights/pretrained_weights.pth\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- 2) Scarica pesi pre-addestrati da Hugging Face ---\n",
    "from huggingface_hub import snapshot_download\n",
    "weights_dir = snapshot_download(\"weighting666/CBraMod\")\n",
    "# cerchiamo un .pth plausibile dentro allo snapshot\n",
    "from glob import glob\n",
    "pths = glob(os.path.join(weights_dir, \"**\", \"*.pth\"), recursive=True)\n",
    "os.makedirs(\"pretrained_weights\", exist_ok=True)\n",
    "target_pth = \"pretrained_weights/pretrained_weights.pth\"\n",
    "if len(pths) > 0:\n",
    "    # copia il primo che troviamo come default\n",
    "    import shutil\n",
    "    shutil.copy(pths[0], target_pth)\n",
    "    print(f\"Checkpoint copiato in: {target_pth}\")\n",
    "else:\n",
    "    print(\"⚠️ Nessun .pth trovato nello snapshot. Se hai un tuo checkpoint, mettilo in pretrained_weights/pretrained_weights.pth\")\n",
    "\n",
    "# --- 3) Import librerie PyTorch e modello ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assicurati che la repo sia nel PYTHONPATH per importare il modello\n",
    "if str(Path.cwd()) not in sys.path:\n",
    "    sys.path.append(str(Path.cwd()))\n",
    "from models.cbramod import CBraMod\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kk-5tJH41Xcu",
    "outputId": "98ed734d-0b48-4a0e-ca5a-fbd2d9db2318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained. Missing keys: 0 Unexpected keys: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CBraMod(\n",
       "  (patch_embedding): PatchEmbedding(\n",
       "    (positional_encoding): Sequential(\n",
       "      (0): Conv2d(200, 200, kernel_size=(19, 7), stride=(1, 1), padding=(9, 3), groups=200)\n",
       "    )\n",
       "    (proj_in): Sequential(\n",
       "      (0): Conv2d(1, 25, kernel_size=(1, 49), stride=(1, 25), padding=(0, 24))\n",
       "      (1): GroupNorm(5, 25, eps=1e-05, affine=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (4): GroupNorm(5, 25, eps=1e-05, affine=True)\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(25, 25, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))\n",
       "      (7): GroupNorm(5, 25, eps=1e-05, affine=True)\n",
       "      (8): GELU(approximate='none')\n",
       "    )\n",
       "    (spectral_proj): Sequential(\n",
       "      (0): Linear(in_features=101, out_features=200, bias=True)\n",
       "      (1): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-11): 12 x TransformerEncoderLayer(\n",
       "        (self_attn_s): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (self_attn_t): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=200, out_features=800, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=800, out_features=200, bias=True)\n",
       "        (norm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Identity()\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 4) Config ---\n",
    "C = 8             # canali\n",
    "S = 4             # segmenti temporali per finestra\n",
    "P = 200           # punti per segmento -> S*P = lunghezza finestra\n",
    "N_CLASSES = 4     # numero classi\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "PATIENCE = 5      # early stopping\n",
    "USE_DUMMY_DATA = True  # True => dati fittizi per smoke test\n",
    "OUTPUT_DIR = Path(\"runs_cbramod\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 5) Dataset ---\n",
    "class MyEEGDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Deve restituire (x, y) con:\n",
    "      x.shape = (C, S, P)\n",
    "      y       = int in [0, N_CLASSES-1]\n",
    "    \"\"\"\n",
    "    def __init__(self, split=\"train\", transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "\n",
    "        if USE_DUMMY_DATA:\n",
    "            # Dati fittizi per prova: 200 esempi train, 50 val, 50 test\n",
    "            if split == \"train\":\n",
    "                self.N = 200\n",
    "            elif split == \"val\":\n",
    "                self.N = 50\n",
    "            else:\n",
    "                self.N = 50\n",
    "            # generiamo tensor (N, C, S, P) e label random\n",
    "            rng = np.random.default_rng(42 + (0 if split==\"train\" else 1 if split==\"val\" else 2))\n",
    "            self.X = rng.normal(size=(self.N, C, S, P)).astype(np.float32)\n",
    "            self.y = rng.integers(low=0, high=N_CLASSES, size=(self.N,), endpoint=False).astype(np.int64)\n",
    "        else:\n",
    "            # TODO: Carica i TUOI dati reali qui.\n",
    "            # Esempio:\n",
    "            # - Leggi file .npy o .mat o .edf preprocessati\n",
    "            # - Prepara X con shape (N, C, S, P) e y (N,)\n",
    "            # - Eventuale normalizzazione z-score per canale\n",
    "            # - Eventuale bilanciamento classi\n",
    "            # Per esempio:\n",
    "            # self.X = np.load(f\"/content/mydata/{split}_X.npy\")  # (N, C, S, P)\n",
    "            # self.y = np.load(f\"/content/mydata/{split}_y.npy\")  # (N,)\n",
    "            # assert self.X.shape[1:] == (C, S, P)\n",
    "            raise NotImplementedError(\"Implementa il caricamento dati reali (metti USE_DUMMY_DATA=False).\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx]  # (C, S, P)\n",
    "        y = int(self.y[idx])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        # converte in torch\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# --- 6) Dataloaders ---\n",
    "train_ds = MyEEGDataset(\"train\")\n",
    "val_ds   = MyEEGDataset(\"val\")\n",
    "test_ds  = MyEEGDataset(\"test\")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# --- 7) Modello + Classifier ---\n",
    "model = CBraMod().to(device)\n",
    "\n",
    "# carica pesi pretrain se presenti\n",
    "if Path(target_pth).exists():\n",
    "    state = torch.load(target_pth, map_location=device)\n",
    "    # carichiamo in modo \"tollerante\"\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(\"Loaded pretrained. Missing keys:\", len(missing), \"Unexpected keys:\", len(unexpected))\n",
    "else:\n",
    "    print(\"⚠️ Nessun checkpoint trovato, si partirà from-scratch.\")\n",
    "\n",
    "# proiezione output a identità per prendere features “grezze”\n",
    "model.proj_out = nn.Identity()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165,
     "referenced_widgets": [
      "daf96ead5c124d809302ef1429c3f617",
      "a36d3cb29101451aa04df9b1b6b3ad07",
      "4ebdb41a05e94315a461932d41de2f50",
      "e181e37d5a4747cd830262cd1ca27641",
      "44f512d44e824c59954047dab826d768",
      "c8827bfac0e2496794547854ad13e51a",
      "c1070fc9b2a14021a9698895eaede3a0",
      "0f5e7bcda78e4ec29af6f8681fa4ce36",
      "08880aa737a542a6bd4ee07d6bc9c097",
      "de0554719aa547c2b0f8548d2153f872",
      "8c3634cf2e6f49e28b61caf1e2b9be08",
      "74f46691f17542d4bd238896cd8c84d4",
      "f07350b1494046caaa83010503657b97",
      "25fbc8f7b4a0441d80bcdd02df8a7455",
      "a53fbd768afe4a03906d9e3f9d4e760a",
      "98154f79c3f64a08857ede024d8359f4",
      "0c87657797804c0593de6a8a4c66b0a3",
      "5ce6a650d05544db856d778e7b95bb04",
      "db33437153b04344bc830d26ef2aea71",
      "07310ced23af4757aff42ec807bd19ce",
      "ba8b4589ffd54e438f614e9d63b5c602",
      "031f0dbcb4fc4652aaa7167c578b19e9",
      "4129a4d69dfe4464ba784b83826d520c",
      "6217db2b738a45589a62a4d8d6340c05",
      "a55d3be7ee73499392449ff1bd5fe12b",
      "9fcb4776ae3843008b5ae5feaa9c7893",
      "a252943d73e742e0bbf7ba756d8540a7",
      "4fc1c2710a6d4543bbcc29ec8ff95920",
      "827267af506f4a60ab676b9fc20f770b",
      "1b650a7ffabb4619b7a0f9d4a59a4eae",
      "31179be90de448e39c9ba1d47dce511d",
      "32192f973a57477d952a2c7606ef5b7f",
      "c494e7adc88f4a8d858f5184cb1659ea"
     ]
    },
    "id": "lHpqJQnAMFvC",
    "outputId": "643c50c0-1add-42ea-9db5-3652e6c8c873"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Extract:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Extract:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Extract:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: (200, 8) (50, 8) (50, 8)\n"
     ]
    }
   ],
   "source": [
    "# ========= Feature extraction per sklearn =========\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.eval()                        # backbone congelato\n",
    "POOLING = \"mean_time\"              # \"mean_time\" | \"flatten\"\n",
    "# \"mean_time\": avg su (S,P) -> feature dim = C\n",
    "# \"flatten\":   vectorizza (C,S,P) -> feature dim = C*S*P (molto più grande)\n",
    "\n",
    "def extract_features(backbone, loader, pooling=\"mean_time\", device=\"cuda\"):\n",
    "    Xs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in tqdm(loader, desc=\"Extract\"):\n",
    "            x = x.to(device)\n",
    "            feats = backbone(x)     # shape (B, C, S, P) perché abbiamo messo model.proj_out = nn.Identity()\n",
    "            if pooling == \"mean_time\":\n",
    "                feats = feats.mean(dim=(2,3))         # (B, C)\n",
    "            elif pooling == \"flatten\":\n",
    "                feats = feats.reshape(feats.size(0), -1)  # (B, C*S*P)\n",
    "            else:\n",
    "                raise ValueError(\"pooling sconosciuto\")\n",
    "            Xs.append(feats.cpu().numpy())\n",
    "            ys.append(y.numpy())\n",
    "    X = np.concatenate(Xs, axis=0)\n",
    "    y = np.concatenate(ys, axis=0)\n",
    "    return X, y\n",
    "\n",
    "X_tr, y_tr = extract_features(model, train_loader, pooling=POOLING, device=device)\n",
    "X_va, y_va = extract_features(model, val_loader,   pooling=POOLING, device=device)\n",
    "X_te, y_te = extract_features(model, test_loader,  pooling=POOLING, device=device)\n",
    "print(\"Shapes:\", X_tr.shape, X_va.shape, X_te.shape)  # es. (200, 8) (50, 8) (50, 8) con mean_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uAMMWok_MN38",
    "outputId": "ed580177-3f52-4a7b-d893-27acc3dd535b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 291, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 259, in _feed\n",
      "    reader_close()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 178, in close\n",
      "    self._close()\n",
      "  File \"/usr/lib/python3.12/multiprocessing/connection.py\", line 377, in _close\n",
      "    _close(self._handle)\n",
      "OSError: [Errno 9] Bad file descriptor\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 291, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA] Val ACC: 0.18\n",
      "[LDA] Test ACC: 0.32\n",
      "Confusion Matrix (LDA):\n",
      " [[ 1  3  9  0]\n",
      " [ 2  3  4  1]\n",
      " [ 2  1 12  0]\n",
      " [ 1  4  7  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "lda_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),                                  # sempre utile\n",
    "    (\"lda\", LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=\"auto\")),\n",
    "])\n",
    "\n",
    "lda_pipe.fit(X_tr, y_tr)\n",
    "va_pred = lda_pipe.predict(X_va)\n",
    "te_pred = lda_pipe.predict(X_te)\n",
    "print(\"[LDA] Val ACC:\", accuracy_score(y_va, va_pred))\n",
    "print(\"[LDA] Test ACC:\", accuracy_score(y_te, te_pred))\n",
    "print(\"Confusion Matrix (LDA):\\n\", confusion_matrix(y_te, te_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTKUwaUEMbEr",
    "outputId": "9a5997ca-8556-4a5a-cd20-7954170b6046"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QDA] Val ACC: 0.3\n",
      "[QDA] Test ACC: 0.38\n",
      "Confusion Matrix (QDA):\n",
      " [[1 7 3 2]\n",
      " [1 5 3 1]\n",
      " [3 3 9 0]\n",
      " [1 3 4 4]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"qda\", QuadraticDiscriminantAnalysis(reg_param=0.1)),  # prova 0.0, 0.1, 0.2...\n",
    "])\n",
    "\n",
    "qda_pipe.fit(X_tr, y_tr)\n",
    "va_pred = qda_pipe.predict(X_va)\n",
    "te_pred = qda_pipe.predict(X_te)\n",
    "print(\"[QDA] Val ACC:\", accuracy_score(y_va, va_pred))\n",
    "print(\"[QDA] Test ACC:\", accuracy_score(y_te, te_pred))\n",
    "print(\"Confusion Matrix (QDA):\\n\", confusion_matrix(y_te, te_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nr4GxgZ_S__u"
   },
   "source": [
    "qui sotto fine tuning totale ma Se volessimo parlare di fine-tuning parziale, significherebbe sbloccare solo alcuni layer del backbone (per esempio gli ultimi due blocchi o solo la testa).\n",
    "Nel mio script puoi farlo modificando questa parte:\n",
    "for name, p in model.named_parameters():\n",
    "    p.requires_grad = (\"block5\" in name)  # esempio: sblocca solo block5\n",
    "ma di default io l’ho messo “tutto sbloccato”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wwR5usR63XDQ",
    "outputId": "fed1b0df-d96d-4cc2-b0e6-f5e234d795c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.280 | Test ACC: 0.300\n",
      "Confusion matrix:\n",
      " [[ 3  0  8  2]\n",
      " [ 3  0  7  0]\n",
      " [ 3  0 10  2]\n",
      " [ 2  0  8  2]]\n"
     ]
    }
   ],
   "source": [
    "# --- (1) Testa PyTorch minimale ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Se prima avevi messo: model.proj_out = nn.Identity()\n",
    "# allora l'output di model(x) è (B, C, S, P). Usiamo Flatten + Linear:\n",
    "classifier = nn.Sequential(\n",
    "    Rearrange('b c s p -> b (c s p)'),\n",
    "    nn.Linear(C*S*P, N_CLASSES),\n",
    ").to(device)\n",
    "\n",
    "# --- (2) Fase A: linear probing \"warmup\" SOLO per la testa ---\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "model.eval()\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):  # poche epoche di warmup\n",
    "    classifier.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.no_grad():        # backbone congelato\n",
    "            feats = model(x)\n",
    "        logits = classifier(feats)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# --- (3) Fase B: fine-tuning PARZIALE: sblocca solo gli ultimi layer ---\n",
    "for name, p in model.named_parameters():\n",
    "    p.requires_grad = (\"block5\" in name or \"block4\" in name)  # <-- adatta al naming reale dei layer\n",
    "\n",
    "# Ora ottimizzi testa + layer sbloccati con LR più bassa\n",
    "ft_params = [p for p in model.parameters() if p.requires_grad] + list(classifier.parameters())\n",
    "optimizer = torch.optim.AdamW(ft_params, lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "EPOCHS = 20\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train(); classifier.train()\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        feats = model(x)\n",
    "        logits = classifier(feats)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# --- (4) Valutazione ---\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "def evaluate(backbone, clf, loader):\n",
    "    backbone.eval(); clf.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            logits = clf(backbone(x))\n",
    "            preds.append(logits.argmax(1).cpu().numpy())\n",
    "            trues.append(y.numpy())\n",
    "    preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "    return accuracy_score(trues, preds), confusion_matrix(trues, preds, labels=list(range(N_CLASSES)))\n",
    "\n",
    "val_acc, _ = evaluate(model, classifier, val_loader)\n",
    "test_acc, cm = evaluate(model, classifier, test_loader)\n",
    "print(f\"Val ACC: {val_acc:.3f} | Test ACC: {test_acc:.3f}\")\n",
    "print(\"Confusion matrix:\\n\", cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3VFzySHAUSmF",
    "outputId": "0a26399c-d183-4d33-e344-7bbe85895c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Monta Google Drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "# Imposta la directory dove si trovano i dati (modifica il percorso secondo la tua struttura)\n",
    "data_dir = \"/content/drive/My Drive/LL2/LL2/raw_data\"\n",
    "\n",
    "# parametri\n",
    "C = 8       # numero di canali nel tuo file\n",
    "S = 4        # segmenti temporali\n",
    "P = 200      # punti per segmento -> finestra = S*P = 800 punti\n",
    "fs = 250     # esempio: 400 Hz\n",
    "window_size = S * P    # punti per finestra\n",
    "stride = window_size   # o puoi usare overlap, tipo window_size//2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "M0_HKwPhUebC"
   },
   "outputs": [],
   "source": [
    "def segment_eeg(df, S=4, P=200):\n",
    "    \"\"\"\n",
    "    Converte un DataFrame EEG (C canali × T punti) in finestre di shape (N, C, S, P)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    T = len(df)\n",
    "    C = df.shape[1] - 1   # escludi la colonna Time\n",
    "    sig = df.iloc[:, :C].values.T  # (C, T)\n",
    "    for start in range(0, T - S*P + 1, S*P):  # nessun overlap\n",
    "        seg = sig[:, start:start + S*P]       # (C, S*P)\n",
    "        seg = seg.reshape(C, S, P)\n",
    "        X.append(seg)\n",
    "    return np.stack(X)  # (N, C, S, P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "13nSXcJmUrhS",
    "outputId": "91b51993-1d00-4cc7-cf46-50fd98f057e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all: (2352, 8, 4, 200) y_all: (2352,)\n"
     ]
    }
   ],
   "source": [
    "tasks = [\"highlevel\", \"midlevel\", \"lowlevel\"]\n",
    "level = [\"Stroop\",\"Arithmetic\"]\n",
    "subjects = range(1, 16)  # esempio 15 soggetti\n",
    "\n",
    "X_all, y_all = [], []\n",
    "for liv in level:\n",
    "    for label, task in enumerate(tasks):\n",
    "        for subj in subjects:\n",
    "            path = os.path.join(data_dir, f\"{liv}_Data\", f\"{task}-{subj}.txt\")\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"[SKIP] File non trovato: {path}\")\n",
    "                continue\n",
    "            df = pd.read_csv(path, sep=\",\", header=None)\n",
    "            # Assumiamo che le prime 15 colonne siano EEG, l'ultima il tempo\n",
    "            df = df.iloc[:, :C+1]\n",
    "            df.columns = [f\"ch{i}\" for i in range(C)] + [\"Time\"]\n",
    "            X_task = segment_eeg(df, S=S, P=P)\n",
    "            X_all.append(X_task)\n",
    "            y_all.extend([label]*len(X_task))\n",
    "\n",
    "X_all = np.concatenate(X_all)\n",
    "y_all = np.array(y_all)\n",
    "print(\"X_all:\", X_all.shape, \"y_all:\", y_all.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RxUPXYpYz2bj"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_all, y_all, test_size=0.3, stratify=y_all, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                        torch.tensor(y_train, dtype=torch.long)), batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                                        torch.tensor(y_val, dtype=torch.long)), batch_size=64)\n",
    "test_loader  = DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                        torch.tensor(y_test, dtype=torch.long)), batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "hHO0NfgW0rXi",
    "outputId": "db0f2fe8-05df-4014-bef6-136d236cfeaf"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1304503003.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproj_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mX_va\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_va\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_te\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean_time\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model.proj_out = nn.Identity()\n",
    "model.eval()\n",
    "X_tr, y_tr = extract_features(model, train_loader, pooling=\"mean_time\", device=device)\n",
    "X_va, y_va = extract_features(model, val_loader,   pooling=\"mean_time\", device=device)\n",
    "X_te, y_te = extract_features(model, test_loader,  pooling=\"mean_time\", device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nHDjJ2B8084x",
    "outputId": "caf516c3-4aff-4e50-f339-ebda71eb45a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LDA] Val ACC: 0.43626062322946174\n",
      "[LDA] Test ACC: 0.43626062322946174\n",
      "Confusion Matrix (LDA):\n",
      " [[153   0   1]\n",
      " [100   0   1]\n",
      " [ 95   2   1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "lda_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),                                  # sempre utile\n",
    "    (\"lda\", LinearDiscriminantAnalysis(solver=\"lsqr\", shrinkage=\"auto\")),\n",
    "])\n",
    "\n",
    "lda_pipe.fit(X_tr, y_tr)\n",
    "va_pred = lda_pipe.predict(X_va)\n",
    "te_pred = lda_pipe.predict(X_te)\n",
    "print(\"[LDA] Val ACC:\", accuracy_score(y_va, va_pred))\n",
    "print(\"[LDA] Test ACC:\", accuracy_score(y_te, te_pred))\n",
    "print(\"Confusion Matrix (LDA):\\n\", confusion_matrix(y_te, te_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "2cda4b0cb4ee41cc9ca3ebb8a99a514f",
      "5570e118c4594c87a849e8ab2b0e0aac",
      "c9c2758ad597468f96eec5f713a682f9",
      "608bbfb330a34d6b8de5389e7ad981cf",
      "f3d59bd28781422e85feb6e8c9b11837",
      "fffc204cd4d54229a48ea08c5d3b78a0",
      "97b02952d1814be7875e3f82b05c0066",
      "01129fc643224fd0b20a5c695c324369",
      "725430f5e8c24433aab0e5d6d74d3393",
      "7b6b1537ff1d46209f3b1beb935d7a24",
      "7ab785d6e1964a4a9c03ace595610133",
      "588e31b839314a53967a580c625c4ce8",
      "7407ee891a444fb59edc0acdf8620304",
      "87661f840938445b95a9b82b668e3ce7",
      "a3284f3352fc437d97c5fc3e167db7b8",
      "a4b64896428a45779347db4b802b8338",
      "230a76a11d144033bbe44632f96b1807",
      "678b0e0c5f4645fabde394a2b1fcebcc",
      "f75cbe94471f4ff783b489c207ec7a69",
      "cc67fb95b4a4436ba5a23699161003d1",
      "8b6f6295a64148cbbcd8638f0f1e62f6",
      "1a0350719ce94d2b9e5a0be9188882a5",
      "5a2a25bab7e7429b923db33a2f0718b1",
      "3ebeb88998f34e25900d58348bdde28c",
      "895ef9e071034a508fc1592cb8274c9d",
      "1e70b1e9baad41a7b78650947610a767",
      "1a4964f1bb8a47d1abcfa267a91cc1d0",
      "704365b1c7e545449a73ab6fb627b7d7",
      "edb97f16c83a441e90c72d09cb844fc8",
      "d69f77851adb4fbd9943bea4557e859f",
      "23f33429bde14ed4a6b6dd9e16ab98e4",
      "a03473b0b03d4af8a57c5b5ee295ad23",
      "874408a5573a40108f4b806b16c111ab",
      "a4ec9e58f2da4efcb3eb42154eca7766",
      "bae9c458ba61403fa76dfc8db239840f",
      "62729afdd6df4e0a87a50c240cee0c30",
      "bbda27a857e74a59b9cdbb4607248af9",
      "8e20c397c6ee4d7c8a33efc2a637340f",
      "7d84e92664a94971841ee311be4fc062",
      "49d218d4ec144e479e598f6de5f2636a",
      "41af2035ba004232ad5c16378f8aa7bd",
      "2c0333450876458a940d85673908aff3",
      "017162f43c6d42e7ba935d836aa8b7fc",
      "f0b948deba07469c852722b96c9dc79a",
      "87a94f0e29f24fecb670a837d53f949f",
      "37e4bd63989b4e10897ca56cdd0ee605",
      "c843b4b0f86a436c9ea49ba8e7623f1b",
      "68fde88d5c634aeda9d361d2868f9369",
      "386047373c9e4fc7a3c54ea2b4ca8d22",
      "e4b3f54565cd4a3c8e698e799c8039c3",
      "bbca3139969544e3968c1ddcf5a94466",
      "d5fdd820e01f47dfbdec0dbdd3ca5126",
      "18d34f7bd14e4b488546175a427b4975",
      "4f3a3c2029214decad4c967b77e3f540",
      "a91930eb8ae343e29731e65d60101d94",
      "c3bf76e7de264fcf9b45d08df4f90098",
      "f362dfcf11184d45b8aa9c0015681f07",
      "d0e760ba0495499fa6e45df4be58d2f0",
      "bac813f9ad8e4010a075f1332e4ec26b",
      "0bb9bd7e2fa04c5f9c785a29824fe21b",
      "f3f4b6358dc14df493783949765b40ef",
      "04d32e072c764f14874724792943236a",
      "7766d0190e8d4506b99844a7dfe8d9fa",
      "c7f6f4523046419fabf8fab30b5b8ce7",
      "4a4d0e15996c47ba935617fa93cac519",
      "155111b907494a74b9c18c62b7cb30df",
      "93eb5e0203fe4e20ab1a4d498f093661",
      "7fcc16b692b149f487e58e02ded2abc9",
      "dafd9da5f8114ed3a34de21da5949fa8",
      "dd9f6c034c9545f38f9692f430d4d888",
      "0d9990c68c1344e68e1e83a2f5178a10",
      "7d94761757c34935abe3402ec2d97f5c",
      "8a4de1fa74004210b2b580eedc81cb0d",
      "46fce0b2dbc94ab78f7fd7965d08c20d",
      "89808947e001428d8992ed9c1c92a3b7",
      "5e6a0e44446d4df390a00b4d991d4e85",
      "9940d600ed334cab946358c7fe99efc8",
      "526720c986a54bef9d2a1b94f84baaec",
      "220b9e30d9b0479799a49f1c896138b2",
      "a71933d2630c40d894546803768c5c78",
      "c63e3277e0224559a7d637eb32bf26b3",
      "d7c257c21feb4041be22dedf449183d7",
      "a88d7fc0c8d5447f82212421a6f70ed6",
      "b9d1ca9ebced4c1ebf91086a9ecdcc5c",
      "faea84d2946548879002f6bba51029bb",
      "17e0d4b9852d417b994d4536dc66fc0a",
      "3f1fb4380d3849b6a797523d67e3fcb2",
      "caea7ddd9fd84c1fa5b8cb1252e0c023",
      "7a546728d9b14d7bb4a22ec33afd2e56",
      "fc8037af276f4de19bde1a6254932595",
      "08bc76d4c51c4cf3bd71aedea6daac5c",
      "74d7fed0b46d455b95fd2bae6484bac8",
      "55245ba9a99942689c128322b4796ee9",
      "a74235d7848743138c25b6c432557877",
      "1e8a520b568042c78165d524166c25fe",
      "1bc85282ad2b4634b0f12bab57063ae3",
      "99c02f3cc43745b6a62fa86f8f3fae54",
      "dcbe83cd25b341f7a5ddc4a98f0d4e56",
      "39ee5330ab684bd2b7ec53505c04bd61",
      "5727e01a5144483c97a214a92bd2fafe",
      "96921faeadd64ede801e12d141e77a7d",
      "a7132daf3dfa48f5a9af18fde3a04dd2",
      "dba4b6fe544342b393b0d4348e4e3c23",
      "74c5640bca7a40ac971e59a0b34cd98f",
      "f7c77442160441d4827da165b98defd2",
      "37c599c8beaa49dca512f1a550b73fe4",
      "f670d1cd69c74d25a6665d32bb3c60ac",
      "3477b1b7d10a4839baf6c501c23ead62",
      "9f5795dc9a8640bba8a3e40d7deeca1f",
      "871d3422e2454b1a9570ad0fb8a985d0",
      "cef6f0fbc2c84f07bef2a8ba4beafa98",
      "d20428166d7e4ed4aadb242ae94ea1cb",
      "f9f8899594b74fc1b80037163c5051e2",
      "cc87486028e146f88731eb8dbb5dc878",
      "32dc78ca88fc4f21b5e7aeb6dc5e993a",
      "c0fc2d191d834a29908d94ea1fc8aa35",
      "a9817505758c4faaa5b72d12f07d2faf",
      "4a7ffd5c940445dea6b09e90ad6d5e1c",
      "1db2a7bc715e468da1a485851006a21c",
      "67231e4df05e40cca753a1fe8a255e4e",
      "574cb6b21c544cb9a95731e92899cfd2"
     ]
    },
    "id": "Bihoe4peSsPt",
    "outputId": "37f112f7-1e3f-425a-ae2f-d85a663059d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained loaded. Missing: 0 | Unexpected: 0\n",
      "\n",
      "=== Warmup: linear probing (backbone congelato) ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Warmup] 1/5:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.3456\n",
      "✅ Miglior warmup salvato: runs_cbramod/best_warmup.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Warmup] 2/5:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.4079\n",
      "✅ Miglior warmup salvato: runs_cbramod/best_warmup.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Warmup] 3/5:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.3031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Warmup] 4/5:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.3003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Warmup] 5/5:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.2946\n",
      "\n",
      "=== Fine-tuning (backbone sbloccato) ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FT] 1/25:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.4051\n",
      "✅ Miglior FT salvato: runs_cbramod/best_finetune.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FT] 2/25:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.2805\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FT] 3/25:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.4363\n",
      "✅ Miglior FT salvato: runs_cbramod/best_finetune.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FT] 4/25:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.3428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FT] 5/25:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val ACC: 0.3513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[FT] 6/25:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-1806937510.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mseen\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrunning\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ===================== 8) Modello   + Fine-tuning =====================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Backbone CBraMod ---\n",
    "backbone = CBraMod().to(device)\n",
    "\n",
    "# Carica pesi pre-addestrati (se presenti)\n",
    "if Path(target_pth).exists():\n",
    "    state = torch.load(target_pth, map_location=device)\n",
    "    missing, unexpected = backbone.load_state_dict(state, strict=False)\n",
    "    print(\"Pretrained loaded. Missing:\", len(missing), \"| Unexpected:\", len(unexpected))\n",
    "else:\n",
    "    print(\"⚠️ Nessun checkpoint pretrain trovato. Si partirà da zero:\", target_pth)\n",
    "\n",
    "# Usiamo l'output \"grezzo\" (stessa shape dell'input): (B, C, S, P)\n",
    "backbone.proj_out = nn.Identity()\n",
    "\n",
    "# --- Testa minimale (Flatten -> Linear) ---\n",
    "class CBMHead(nn.Module):\n",
    "    def __init__(self, C, S, P, n_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(C*S*P, n_classes)\n",
    "    def forward(self, feats):                  # feats: (B, C, S, P)\n",
    "        return self.fc(feats.reshape(feats.size(0), -1))\n",
    "\n",
    "head = CBMHead(C, S, P, N_CLASSES).to(device)\n",
    "\n",
    "# --- Helper per forward + valutazione ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def forward_logits(x):\n",
    "    x = x.to(device).float()                   # (B, C, S, P)\n",
    "    feats = backbone(x)                        # (B, C, S, P) perché proj_out = Identity\n",
    "    logits = head(feats)                       # (B, N_CLASSES)\n",
    "    return logits\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(loader):\n",
    "    backbone.eval(); head.eval()\n",
    "    preds, trues = [], []\n",
    "    for xb, yb in loader:\n",
    "        logits = forward_logits(xb)\n",
    "        preds.append(logits.argmax(1).cpu().numpy())\n",
    "        trues.append(yb.numpy())\n",
    "    preds = np.concatenate(preds); trues = np.concatenate(trues)\n",
    "    return accuracy_score(trues, preds), confusion_matrix(trues, preds, labels=list(range(N_CLASSES)))\n",
    "\n",
    "# --- Hyperparam (puoi modificarli a piacere) ---\n",
    "EPOCHS_WARMUP = 5\n",
    "EPOCHS_FT     = 25\n",
    "LR_WARMUP     = 1e-3\n",
    "LR_FT         = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "PATIENCE      = 6\n",
    "\n",
    "# --- 8A) Warmup: Linear probing (backbone congelato, allena SOLO la testa) ---\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "backbone.eval()\n",
    "\n",
    "opt = torch.optim.AdamW(head.parameters(), lr=LR_WARMUP, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_val = -1.0\n",
    "epochs_no_improve = 0\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "best_warmup_path = OUTPUT_DIR / \"best_warmup.pt\"\n",
    "\n",
    "print(\"\\n=== Warmup: linear probing (backbone congelato) ===\")\n",
    "for epoch in range(1, EPOCHS_WARMUP+1):\n",
    "    head.train()\n",
    "    running, seen = 0.0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"[Warmup] {epoch}/{EPOCHS_WARMUP}\")\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(device).float(), yb.to(device).long()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = forward_logits(xb)            # backbone in eval, ma forward ok\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        bs = xb.size(0); running += loss.item()*bs; seen += bs\n",
    "        pbar.set_postfix(loss=running/max(1,seen))\n",
    "    val_acc, _ = evaluate(val_loader)\n",
    "    print(f\"Val ACC: {val_acc:.4f}\")\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc; epochs_no_improve = 0\n",
    "        torch.save({\"head\": head.state_dict()}, best_warmup_path)\n",
    "        print(\"✅ Miglior warmup salvato:\", best_warmup_path)\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"⏹️ Early stop (warmup).\")\n",
    "            break\n",
    "\n",
    "# Ricarica la migliore testa del warmup\n",
    "if best_warmup_path.exists():\n",
    "    ck = torch.load(best_warmup_path, map_location=device)\n",
    "    head.load_state_dict(ck[\"head\"])\n",
    "\n",
    "# --- 8B) Fine-tuning: sblocca il backbone (totale) e continua ad allenare testa+backbone ---\n",
    "# Se vuoi un fine-tuning PARZIALE, qui potresti sbloccare solo alcuni layer con name-filter.\n",
    "for p in backbone.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "opt = torch.optim.AdamW(list(backbone.parameters()) + list(head.parameters()),\n",
    "                        lr=LR_FT, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "best_val = -1.0\n",
    "epochs_no_improve = 0\n",
    "best_ft_path = OUTPUT_DIR / \"best_finetune.pt\"\n",
    "\n",
    "print(\"\\n=== Fine-tuning (backbone sbloccato) ===\")\n",
    "for epoch in range(1, EPOCHS_FT+1):\n",
    "    backbone.train(); head.train()\n",
    "    running, seen = 0.0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"[FT] {epoch}/{EPOCHS_FT}\")\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(device).float(), yb.to(device).long()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = forward_logits(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        bs = xb.size(0); running += loss.item()*bs; seen += bs\n",
    "        pbar.set_postfix(loss=running/max(1,seen))\n",
    "\n",
    "    val_acc, _ = evaluate(val_loader)\n",
    "    print(f\"Val ACC: {val_acc:.4f}\")\n",
    "    if val_acc > best_val:\n",
    "        best_val = val_acc; epochs_no_improve = 0\n",
    "        torch.save({\"backbone\": backbone.state_dict(),\n",
    "                    \"head\": head.state_dict()}, best_ft_path)\n",
    "        print(\"✅ Miglior FT salvato:\", best_ft_path)\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"⏹️ Early stop (FT).\")\n",
    "            break\n",
    "\n",
    "# --- 9) Test finale ---\n",
    "if best_ft_path.exists():\n",
    "    ck = torch.load(best_ft_path, map_location=device)\n",
    "    backbone.load_state_dict(ck[\"backbone\"], strict=False)\n",
    "    head.load_state_dict(ck[\"head\"])\n",
    "\n",
    "test_acc, cm = evaluate(test_loader)\n",
    "print(\"\\n=== RISULTATI FINALI ===\")\n",
    "print(f\"Test ACC: {test_acc:.4f}\")\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Report dettagliato\n",
    "@torch.no_grad()\n",
    "def collect_preds(loader):\n",
    "    backbone.eval(); head.eval()\n",
    "    yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        logits = forward_logits(xb)\n",
    "        yp.append(logits.argmax(1).cpu().numpy())\n",
    "        yt.append(yb.numpy())\n",
    "    return np.concatenate(yp), np.concatenate(yt)\n",
    "\n",
    "yhat, ytrue = collect_preds(test_loader)\n",
    "print(classification_report(ytrue, yhat, digits=3))\n",
    "print(\"Artefatti salvati in:\", OUTPUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 910
    },
    "id": "B2tHfWmIWn5T",
    "outputId": "71ccbea8-6fa0-40ef-c570-3483a99107fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "/content/EEGPT\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m See above for output.\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
      "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py:679: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(True)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py:693: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Canali passati a EEGPT: ['Fp1', 'Fp2', 'F7', 'F3', 'FZ', 'F4', 'F8', 'C2']\n",
      "Dataset dummy finestrato: (440, 8, 1024) (440,)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3618730253.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0muse_pretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m model = EEGPTClassifier(\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_pretrained\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, in_channels, img_size, patch_stride, use_channels_names, use_mean_pooling, norm_layer, use_chan_conv, max_norm_chan_conv, max_norm_head, qkv_bias, enc_drop_rate, enc_attn_drop_rate, enc_drop_path_rate, rec_drop_rate, rec_attn_drop_rate, rec_drop_path_rate, use_freeze_encoder, use_freeze_reconstructor, interpolate_factor, desired_time_len, use_avg, use_predictor, use_out_proj, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mreconstructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchans_id\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtarget_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chan_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_channels_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0muse_predictor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_out_proj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py\u001b[0m in \u001b[0;36mprepare_chan_ids\u001b[0;34m(self, channels)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_chan_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mchan_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m             \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCHANNEL_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# ================== EEGPT con OpenBCI (8 canali @250 Hz) ==================\n",
    "!pip -q install einops tqdm scikit-learn scipy\n",
    "\n",
    "import os, sys, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from scipy.signal import resample_poly\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# --- Repo EEGPT ---\n",
    "%cd /content\n",
    "if not Path(\"EEGPT\").exists():\n",
    "    !git clone -q https://github.com/BINE022/EEGPT.git\n",
    "%cd EEGPT\n",
    "!pip -q install -r requirements.txt\n",
    "\n",
    "# --- Import modello EEGPT ---\n",
    "if str(Path.cwd()) not in sys.path:\n",
    "    sys.path.append(str(Path.cwd()))\n",
    "from downstream.Modules.models.EEGPT_mcae_finetune import EEGPTClassifier, CHANNEL_DICT\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# --------- 1) Config canali & sampling ----------\n",
    "# I TUOI CANALI (OpenBCI 8ch)\n",
    "my_channels = [\"Fp1\",\"Fp2\",\"F7\",\"F3\",\"FZ\",\"F4\",\"F8\",\"C2\"]  # case-insensitive\n",
    "# Verifica che siano nel dizionario del repo (usa maiuscole interne)\n",
    "channels = []\n",
    "for ch in my_channels:\n",
    "    key = ch.upper()\n",
    "    if key not in CHANNEL_DICT:\n",
    "        raise ValueError(f\"Canale non riconosciuto da EEGPT: {ch}. Controlla la nomenclatura 10-20.\")\n",
    "    channels.append(ch)  # tieni l'originale; il wrapper fa upper() da solo\n",
    "\n",
    "print(\"Canali passati a EEGPT:\", channels)\n",
    "\n",
    "FS_SRC = 250    # OpenBCI\n",
    "FS_TGT = 256    # EEGPT pretrain\n",
    "WIN_SEC = 4.0   # 4 secondi\n",
    "TGT_LEN = int(FS_TGT * WIN_SEC)  # 1024 campioni\n",
    "STRIDE_SEC = 2.0  # overlap 50%\n",
    "STRIDE = int(FS_TGT * STRIDE_SEC)\n",
    "\n",
    "# --------- 2) Funzioni: resample 250->256 e segmentazione (C,T)->(N,C,1024) ----------\n",
    "def resample_to_256(x_8xT, fs_src=250, fs_tgt=256):\n",
    "    # x shape: (C=8, T)\n",
    "    # usa resample_poly: fattori piccoli e precisi (up=128, down=125 per 250->256)\n",
    "    up, down = 128, 125\n",
    "    return resample_poly(x_8xT, up, down, axis=1)\n",
    "\n",
    "def make_windows(x_8xT_256, win_len=TGT_LEN, stride=STRIDE):\n",
    "    # x: (C=8, T256)\n",
    "    C, T = x_8xT_256.shape\n",
    "    xs = []\n",
    "    for start in range(0, T - win_len + 1, stride):\n",
    "        seg = x_8xT_256[:, start:start+win_len]  # (8,1024)\n",
    "        xs.append(seg)\n",
    "    return np.stack(xs, axis=0) if xs else None  # (N, 8, 1024)\n",
    "\n",
    "# --------- 3) Qui simulo il CARICAMENTO dei tuoi dati grezzi ---------\n",
    "# Sostituisci questa parte con il tuo loader reale (file .csv/.txt/.edf già sincronizzati).\n",
    "# Creo un dummy \"per sessione\": 90 s @250 Hz -> 22500 campioni\n",
    "rng = np.random.default_rng(42)\n",
    "def make_dummy_session(seconds=90, n_classes=4):\n",
    "    T = int(FS_SRC * seconds)\n",
    "    x = rng.normal(size=(8, T)).astype(np.float32)           # (8, T250)\n",
    "    y_label = rng.integers(0, n_classes)                     # un'etichetta per la sessione (esempio)\n",
    "    return x, y_label\n",
    "\n",
    "# Costruisco dataset fittizio (10 sessioni -> poi segmentate in finestre)\n",
    "N_SESS = 10\n",
    "N_CLASSES = 4\n",
    "X_all, y_all = [], []\n",
    "for _ in range(N_SESS):\n",
    "    raw, y = make_dummy_session(seconds=90, n_classes=N_CLASSES)     # (8, T250)\n",
    "    raw_256 = resample_to_256(raw)                                   # (8, T256)\n",
    "    Xw = make_windows(raw_256)                                       # (N, 8, 1024)\n",
    "    if Xw is None:\n",
    "        continue\n",
    "    X_all.append(Xw)\n",
    "    y_all += [y]*len(Xw)\n",
    "\n",
    "X_all = np.concatenate(X_all, axis=0)  # (N, 8, 1024)\n",
    "y_all = np.array(y_all, dtype=np.int64)\n",
    "print(\"Dataset dummy finestrato:\", X_all.shape, y_all.shape)\n",
    "\n",
    "# --------- 4) Split e DataLoader ----------\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_all, test_size=0.2, stratify=y_all, random_state=0)\n",
    "X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.25, stratify=y_tr, random_state=0)  # 60/20/20\n",
    "\n",
    "BATCH = 64\n",
    "train_loader = DataLoader(TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr)), batch_size=BATCH, shuffle=True,  pin_memory=True)\n",
    "val_loader   = DataLoader(TensorDataset(torch.tensor(X_va), torch.tensor(y_va)), batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "test_loader  = DataLoader(TensorDataset(torch.tensor(X_te), torch.tensor(y_te)), batch_size=BATCH, shuffle=False, pin_memory=True)\n",
    "\n",
    "# --------- 5) Modello EEGPT con i TUOI 8 canali ----------\n",
    "ckpt_path = Path(\"checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\")  # metti qui il ckpt se ce l'hai\n",
    "use_pretrained = ckpt_path.exists()\n",
    "\n",
    "model = EEGPTClassifier(\n",
    "    num_classes=N_CLASSES,\n",
    "    ckpt_path=str(ckpt_path) if use_pretrained else None,\n",
    "    channels=channels,        # <<<<<< 8 canali: il resto è \"mascherato\"\n",
    "    sample_rate=FS_TGT,       # 256 Hz (dopo il resample)\n",
    "    input_time_length=int(WIN_SEC)\n",
    ").to(device)\n",
    "print(\"EEGPT init OK. Pretrained:\", use_pretrained)\n",
    "\n",
    "# --------- 6) Linear probing (congela il backbone) ----------\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# EEGPTClassifier in genere restituisce direttamente i logits\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "def forward_logits(xb):\n",
    "    xb = xb.to(device).float()  # (B, 8, 1024)\n",
    "    out = model(xb)\n",
    "    if isinstance(out, dict) and \"logits\" in out:\n",
    "        out = out[\"logits\"]\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_loader(loader):\n",
    "    model.eval()\n",
    "    yp, yt = [], []\n",
    "    for xb, yb in loader:\n",
    "        logits = forward_logits(xb)\n",
    "        yp.append(logits.argmax(1).cpu().numpy())\n",
    "        yt.append(yb.numpy())\n",
    "    yp = np.concatenate(yp); yt = np.concatenate(yt)\n",
    "    return float(accuracy_score(yt, yp)), confusion_matrix(yt, yp, labels=list(range(N_CLASSES)))\n",
    "\n",
    "EPOCHS, PATIENCE = 5, 3\n",
    "best_val, noimp = -1.0, 0\n",
    "print(\"\\n=== Linear Probing (backbone congelato) ===\")\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    run, seen = 0.0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"[LP] {ep}/{EPOCHS}\")\n",
    "    for xb, yb in pbar:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        logits = forward_logits(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward(); opt.step()\n",
    "        bs = xb.size(0); run += loss.item()*bs; seen += bs\n",
    "        pbar.set_postfix(loss=run/max(1,seen))\n",
    "    va_acc, _ = eval_loader(val_loader)\n",
    "    print(f\"Val ACC: {va_acc:.3f}\")\n",
    "    if va_acc > best_val: best_val, noimp = va_acc, 0\n",
    "    else:\n",
    "        noimp += 1\n",
    "        if noimp >= PATIENCE:\n",
    "            print(\"Early stop.\")\n",
    "            break\n",
    "\n",
    "# --------- 7) Test ----------\n",
    "te_acc, cm = eval_loader(test_loader)\n",
    "print(\"\\n=== TEST ===\")\n",
    "print(\"Test ACC:\", te_acc)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "print(classification_report(\n",
    "    np.concatenate([yb.numpy() for _, yb in test_loader]),\n",
    "    np.concatenate([forward_logits(xb).argmax(1).cpu().numpy() for xb, _ in test_loader]),\n",
    "    digits=3\n",
    "))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFT4SLK_PrMi"
   },
   "source": [
    "# domande\n",
    "\n",
    "- serve un channel adapter ?\n",
    "-"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOe7bSw80BKY0sc1wgSCndj",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
