{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbZL5Ve/Y4FvCgP0DkGlFK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/camillabocciolone/Leonardo-project/blob/main/ll2egpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02NjSz-dDD0I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# proviamo EEGPT con OpenBCI"
      ],
      "metadata": {
        "id": "NhCBK-Qs7VVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import libraries"
      ],
      "metadata": {
        "id": "Fn95vR8gDch1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install einops tqdm scikit-learn scipy\n",
        "\n",
        "import os, sys, numpy as np, torch\n",
        "from pathlib import Path\n",
        "from scipy.signal import resample_poly\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n"
      ],
      "metadata": {
        "id": "sc1WV80ODZAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## --- Repo EEGPT ---"
      ],
      "metadata": {
        "id": "81ukh1v7Ekb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/EEGPT\n",
        "!grep -vE '^av==' requirements.txt > requirements_noav.txt\n",
        "!pip install -r requirements_noav.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwkBYSQfFRup",
        "outputId": "ac12a380-22d6-4d05-a3be-0b366281fcf0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/EEGPT\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_noav.txt (line 1)) (1.4.0)\n",
            "Collecting accelerate==0.25.0 (from -r requirements_noav.txt (line 2))\n",
            "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting adan-pytorch==0.1.0 (from -r requirements_noav.txt (line 3))\n",
            "  Using cached adan_pytorch-0.1.0-py3-none-any.whl.metadata (661 bytes)\n",
            "Collecting aiohttp==3.8.4 (from -r requirements_noav.txt (line 4))\n",
            "  Using cached aiohttp-3.8.4.tar.gz (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiosignal==1.3.1 (from -r requirements_noav.txt (line 5))\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting albumentations==1.4.1 (from -r requirements_noav.txt (line 6))\n",
            "  Using cached albumentations-1.4.1-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting alembic==1.10.3 (from -r requirements_noav.txt (line 7))\n",
            "  Using cached alembic-1.10.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting appdirs==1.4.4 (from -r requirements_noav.txt (line 8))\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting async-timeout==4.0.2 (from -r requirements_noav.txt (line 9))\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting attrs==23.1.0 (from -r requirements_noav.txt (line 10))\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting audioread==3.0.1 (from -r requirements_noav.txt (line 11))\n",
            "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting axial-positional-embedding==0.2.1 (from -r requirements_noav.txt (line 12))\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bleach==6.1.0 (from -r requirements_noav.txt (line 13))\n",
            "  Downloading bleach-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting braindecode==0.8.1 (from -r requirements_noav.txt (line 14))\n",
            "  Downloading braindecode-0.8.1-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting cachetools==5.3.1 (from -r requirements_noav.txt (line 15))\n",
            "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting cffi==1.16.0 (from -r requirements_noav.txt (line 16))\n",
            "  Downloading cffi-1.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting cftime==1.6.3 (from -r requirements_noav.txt (line 17))\n",
            "  Downloading cftime-1.6.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.6 kB)\n",
            "Collecting charset-normalizer==3.1.0 (from -r requirements_noav.txt (line 18))\n",
            "  Downloading charset_normalizer-3.1.0-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting cmaes==0.9.1 (from -r requirements_noav.txt (line 19))\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cmake==3.26.1 (from -r requirements_noav.txt (line 20))\n",
            "  Downloading cmake-3.26.1-py2.py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting colorama==0.4.6 (from -r requirements_noav.txt (line 21))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting colorlog==6.7.0 (from -r requirements_noav.txt (line 22))\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting CoLT5-attention==0.10.20 (from -r requirements_noav.txt (line 23))\n",
            "  Downloading CoLT5_attention-0.10.20-py3-none-any.whl.metadata (738 bytes)\n",
            "Collecting configparser==6.0.0 (from -r requirements_noav.txt (line 24))\n",
            "  Downloading configparser-6.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting contourpy==1.0.7 (from -r requirements_noav.txt (line 25))\n",
            "  Downloading contourpy-1.0.7.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m115.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting coverage==7.3.2 (from -r requirements_noav.txt (line 26))\n",
            "  Downloading coverage-7.3.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting cramjam==2.8.1 (from -r requirements_noav.txt (line 27))\n",
            "  Downloading cramjam-2.8.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting cycler==0.11.0 (from -r requirements_noav.txt (line 28))\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl.metadata (785 bytes)\n",
            "Collecting decord==0.6.0 (from -r requirements_noav.txt (line 29))\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: dill==0.3.8 in /usr/local/lib/python3.12/dist-packages (from -r requirements_noav.txt (line 30)) (0.3.8)\n",
            "Collecting docstring-inheritance==2.0.2 (from -r requirements_noav.txt (line 31))\n",
            "  Downloading docstring_inheritance-2.0.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting easydev==0.12.1 (from -r requirements_noav.txt (line 32))\n",
            "  Downloading easydev-0.12.1.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting EDFlib-Python==1.0.8 (from -r requirements_noav.txt (line 33))\n",
            "  Downloading EDFlib_Python-1.0.8-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting einops==0.7.0 (from -r requirements_noav.txt (line 34))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting et-xmlfile==1.1.0 (from -r requirements_noav.txt (line 35))\n",
            "  Downloading et_xmlfile-1.1.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting exceptiongroup==1.1.3 (from -r requirements_noav.txt (line 36))\n",
            "  Downloading exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting fastparquet==2024.2.0 (from -r requirements_noav.txt (line 37))\n",
            "  Downloading fastparquet-2024.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting filelock==3.11.0 (from -r requirements_noav.txt (line 38))\n",
            "  Downloading filelock-3.11.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting fire==0.5.0 (from -r requirements_noav.txt (line 39))\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fonttools==4.39.3 (from -r requirements_noav.txt (line 40))\n",
            "  Downloading fonttools-4.39.3-py3-none-any.whl.metadata (145 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.7/145.7 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist==1.3.3 (from -r requirements_noav.txt (line 41))\n",
            "  Downloading frozenlist-1.3.3.tar.gz (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fsspec==2023.5.0 (from -r requirements_noav.txt (line 42))\n",
            "  Downloading fsspec-2023.5.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting fvcore==0.1.5.post20221221 (from -r requirements_noav.txt (line 43))\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-auth==2.22.0 (from -r requirements_noav.txt (line 44))\n",
            "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting google-auth-oauthlib==1.0.0 (from -r requirements_noav.txt (line 45))\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting greenlet==2.0.2 (from -r requirements_noav.txt (line 46))\n",
            "  Downloading greenlet-2.0.2.tar.gz (164 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.0/165.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting grpcio==1.56.0 (from -r requirements_noav.txt (line 47))\n",
            "  Downloading grpcio-1.56.0.tar.gz (24.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h5py==3.8.0 (from -r requirements_noav.txt (line 48))\n",
            "  Downloading h5py-3.8.0.tar.gz (400 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.8/400.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hjson==3.1.0 (from -r requirements_noav.txt (line 49))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting huggingface-hub==0.19.4 (from -r requirements_noav.txt (line 50))\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting idna==3.4 (from -r requirements_noav.txt (line 51))\n",
            "  Downloading idna-3.4-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting imageio==2.28.1 (from -r requirements_noav.txt (line 52))\n",
            "  Downloading imageio-2.28.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting iniconfig==2.0.0 (from -r requirements_noav.txt (line 53))\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting iopath==0.1.10 (from -r requirements_noav.txt (line 54))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting Jinja2==3.1.2 (from -r requirements_noav.txt (line 55))\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting joblib==1.2.0 (from -r requirements_noav.txt (line 56))\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting kaggle==1.6.6 (from -r requirements_noav.txt (line 57))\n",
            "  Downloading kaggle-1.6.6.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.6/84.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kiwisolver==1.4.4 (from -r requirements_noav.txt (line 58))\n",
            "  Downloading kiwisolver-1.4.4.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.1/97.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kymatio==0.3.0 (from -r requirements_noav.txt (line 59))\n",
            "  Downloading kymatio-0.3.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting lazy_loader==0.3 (from -r requirements_noav.txt (line 60))\n",
            "  Downloading lazy_loader-0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting librosa==0.10.1 (from -r requirements_noav.txt (line 61))\n",
            "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting lightning-utilities==0.8.0 (from -r requirements_noav.txt (line 62))\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting linear-attention-transformer==0.19.1 (from -r requirements_noav.txt (line 63))\n",
            "  Downloading linear_attention_transformer-0.19.1-py3-none-any.whl.metadata (787 bytes)\n",
            "Collecting linformer==0.2.3 (from -r requirements_noav.txt (line 64))\n",
            "  Downloading linformer-0.2.3-py3-none-any.whl.metadata (602 bytes)\n",
            "Collecting lion-pytorch==0.1.2 (from -r requirements_noav.txt (line 65))\n",
            "  Downloading lion_pytorch-0.1.2-py3-none-any.whl.metadata (620 bytes)\n",
            "Collecting lit==16.0.0 (from -r requirements_noav.txt (line 66))\n",
            "  Downloading lit-16.0.0.tar.gz (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting llvmlite==0.42.0 (from -r requirements_noav.txt (line 67))\n",
            "  Downloading llvmlite-0.42.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting lmdb==1.4.1 (from -r requirements_noav.txt (line 68))\n",
            "  Downloading lmdb-1.4.1.tar.gz (881 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.5/881.5 kB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting local-attention==1.9.1 (from -r requirements_noav.txt (line 69))\n",
            "  Downloading local_attention-1.9.1-py3-none-any.whl.metadata (682 bytes)\n",
            "Collecting Mako==1.2.4 (from -r requirements_noav.txt (line 70))\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting Markdown==3.4.3 (from -r requirements_noav.txt (line 71))\n",
            "  Downloading Markdown-3.4.3-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting MarkupSafe==2.1.2 (from -r requirements_noav.txt (line 72))\n",
            "  Downloading MarkupSafe-2.1.2.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib==3.7.1 (from -r requirements_noav.txt (line 73))\n",
            "  Downloading matplotlib-3.7.1.tar.gz (38.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting medmnist==2.2.2 (from -r requirements_noav.txt (line 74))\n",
            "  Downloading medmnist-2.2.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting memory-profiler==0.61.0 (from -r requirements_noav.txt (line 75))\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting mne==1.4.2 (from -r requirements_noav.txt (line 76))\n",
            "  Downloading mne-1.4.2-py3-none-any.whl.metadata (8.3 kB)\n",
            "Collecting mne-bids==0.13 (from -r requirements_noav.txt (line 77))\n",
            "  Downloading mne_bids-0.13-py2.py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting mne-connectivity==0.5.0 (from -r requirements_noav.txt (line 78))\n",
            "  Downloading mne_connectivity-0.5.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "\u001b[31mERROR: Ignored the following yanked versions: 1.4.1\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following versions that require a different python version: 0.1 Requires-Python <3.11,>=3.6; 0.36.0 Requires-Python >=3.6,<3.10; 0.37.0 Requires-Python >=3.7,<3.10; 0.38.0 Requires-Python >=3.7,<3.11; 0.38.1 Requires-Python >=3.7,<3.11; 0.5.0 Requires-Python >=3.8,<3.11; 1.0.0 Requires-Python <3.11,>=3.6; 1.0.0 Requires-Python >=3.8,<3.12; 1.0.1 Requires-Python <3.12,>=3.7; 1.1.0 Requires-Python <3.12,>=3.7; 1.1.1 Requires-Python <3.12,>=3.7; 2.0.0 Requires-Python <3.12,>=3.8; 2.0.0rc0 Requires-Python <3.12,>=3.8; 2.0.1 Requires-Python <3.12,>=3.8\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement moabb==1.0.0 (from versions: 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 1.1.0, 1.1.1, 1.2.0, 1.4.0, 1.4.2, 1.4.3)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for moabb==1.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd /content\n",
        "if not Path(\"EEGPT\").exists():\n",
        "    !git clone -q https://github.com/BINE022/EEGPT.git\n",
        "%cd EEGPT\n",
        "!pip -q in\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQxm4s5AEjyg",
        "outputId": "f441928a-c473-45a0-87ed-d95cff8bea31"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/EEGPT\n",
            "ERROR: unknown command \"in\"\n",
            "Requirement already satisfied: absl-py==1.4.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 1)) (1.4.0)\n",
            "Collecting accelerate==0.25.0 (from -r requirements.txt (line 2))\n",
            "  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting adan-pytorch==0.1.0 (from -r requirements.txt (line 3))\n",
            "  Using cached adan_pytorch-0.1.0-py3-none-any.whl.metadata (661 bytes)\n",
            "Collecting aiohttp==3.8.4 (from -r requirements.txt (line 4))\n",
            "  Using cached aiohttp-3.8.4.tar.gz (7.3 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 5))\n",
            "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting albumentations==1.4.1 (from -r requirements.txt (line 6))\n",
            "  Using cached albumentations-1.4.1-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting alembic==1.10.3 (from -r requirements.txt (line 7))\n",
            "  Using cached alembic-1.10.3-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting appdirs==1.4.4 (from -r requirements.txt (line 8))\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting async-timeout==4.0.2 (from -r requirements.txt (line 9))\n",
            "  Using cached async_timeout-4.0.2-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting attrs==23.1.0 (from -r requirements.txt (line 10))\n",
            "  Using cached attrs-23.1.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting audioread==3.0.1 (from -r requirements.txt (line 11))\n",
            "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting av==10.0.0 (from -r requirements.txt (line 12))\n",
            "  Using cached av-10.0.0.tar.gz (2.4 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "id": "B2tHfWmIWn5T",
        "outputId": "d387807e-8fe3-46a2-e8b0-56cec521fed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/EEGPT\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py:679: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(True)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py:693: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  @autocast(True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n",
            "Canali passati a EEGPT: ['Fp1', 'Fp2', 'F7', 'F3', 'FZ', 'F4', 'F8', 'C2']\n",
            "Dataset dummy finestrato: (440, 8, 1024) (440,)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not iterable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3927444486.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0muse_pretrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m model = EEGPTClassifier(\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_CLASSES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0muse_pretrained\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_classes, in_channels, img_size, patch_stride, use_channels_names, use_mean_pooling, norm_layer, use_chan_conv, max_norm_chan_conv, max_norm_head, qkv_bias, enc_drop_rate, enc_attn_drop_rate, enc_drop_path_rate, rec_drop_rate, rec_attn_drop_rate, rec_drop_path_rate, use_freeze_encoder, use_freeze_reconstructor, interpolate_factor, desired_time_len, use_avg, use_predictor, use_out_proj, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mreconstructor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchans_id\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtarget_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_chan_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_channels_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0muse_predictor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0muse_out_proj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/EEGPT/downstream/Modules/models/EEGPT_mcae_finetune.py\u001b[0m in \u001b[0;36mprepare_chan_ids\u001b[0;34m(self, channels)\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprepare_chan_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mchan_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m             \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCHANNEL_DICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
          ]
        }
      ],
      "source": [
        "\n",
        "stall -r requirements.txt\n",
        "\n",
        "# --- Import modello EEGPT ---\n",
        "if str(Path.cwd()) not in sys.path:\n",
        "    sys.path.append(str(Path.cwd()))\n",
        "from downstream.Modules.models.EEGPT_mcae_finetune import EEGPTClassifier, CHANNEL_DICT\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# --------- 1) Config canali & sampling ----------\n",
        "# I TUOI CANALI (OpenBCI 8ch)\n",
        "my_channels = [\"Fp1\",\"Fp2\",\"F7\",\"F3\",\"FZ\",\"F4\",\"F8\",\"C2\"]  # case-insensitive\n",
        "# Verifica che siano nel dizionario del repo (usa maiuscole interne)\n",
        "channels = []\n",
        "for ch in my_channels:\n",
        "    key = ch.upper()\n",
        "    if key not in CHANNEL_DICT:\n",
        "        raise ValueError(f\"Canale non riconosciuto da EEGPT: {ch}. Controlla la nomenclatura 10-20.\")\n",
        "    channels.append(ch)  # tieni l'originale; il wrapper fa upper() da solo\n",
        "\n",
        "print(\"Canali passati a EEGPT:\", channels)\n",
        "\n",
        "FS_SRC = 250    # OpenBCI\n",
        "FS_TGT = 256    # EEGPT pretrain\n",
        "WIN_SEC = 4.0   # 4 secondi\n",
        "TGT_LEN = int(FS_TGT * WIN_SEC)  # 1024 campioni\n",
        "STRIDE_SEC = 2.0  # overlap 50%\n",
        "STRIDE = int(FS_TGT * STRIDE_SEC)\n",
        "\n",
        "# --------- 2) Funzioni: resample 250->256 e segmentazione (C,T)->(N,C,1024) ----------\n",
        "def resample_to_256(x_8xT, fs_src=250, fs_tgt=256):\n",
        "    # x shape: (C=8, T)\n",
        "    # usa resample_poly: fattori piccoli e precisi (up=128, down=125 per 250->256)\n",
        "    up, down = 128, 125\n",
        "    return resample_poly(x_8xT, up, down, axis=1)\n",
        "\n",
        "def make_windows(x_8xT_256, win_len=TGT_LEN, stride=STRIDE):\n",
        "    # x: (C=8, T256)\n",
        "    C, T = x_8xT_256.shape\n",
        "    xs = []\n",
        "    for start in range(0, T - win_len + 1, stride):\n",
        "        seg = x_8xT_256[:, start:start+win_len]  # (8,1024)\n",
        "        xs.append(seg)\n",
        "    return np.stack(xs, axis=0) if xs else None  # (N, 8, 1024)\n",
        "\n",
        "# --------- 3) Qui simulo il CARICAMENTO dei tuoi dati grezzi ---------\n",
        "# Sostituisci questa parte con il tuo loader reale (file .csv/.txt/.edf già sincronizzati).\n",
        "# Creo un dummy \"per sessione\": 90 s @250 Hz -> 22500 campioni\n",
        "rng = np.random.default_rng(42)\n",
        "def make_dummy_session(seconds=90, n_classes=4):\n",
        "    T = int(FS_SRC * seconds)\n",
        "    x = rng.normal(size=(8, T)).astype(np.float32)           # (8, T250)\n",
        "    y_label = rng.integers(0, n_classes)                     # un'etichetta per la sessione (esempio)\n",
        "    return x, y_label\n",
        "\n",
        "# Costruisco dataset fittizio (10 sessioni -> poi segmentate in finestre)\n",
        "N_SESS = 10\n",
        "N_CLASSES = 4\n",
        "X_all, y_all = [], []\n",
        "for _ in range(N_SESS):\n",
        "    raw, y = make_dummy_session(seconds=90, n_classes=N_CLASSES)     # (8, T250)\n",
        "    raw_256 = resample_to_256(raw)                                   # (8, T256)\n",
        "    Xw = make_windows(raw_256)                                       # (N, 8, 1024)\n",
        "    if Xw is None:\n",
        "        continue\n",
        "    X_all.append(Xw)\n",
        "    y_all += [y]*len(Xw)\n",
        "\n",
        "X_all = np.concatenate(X_all, axis=0)  # (N, 8, 1024)\n",
        "y_all = np.array(y_all, dtype=np.int64)\n",
        "print(\"Dataset dummy finestrato:\", X_all.shape, y_all.shape)\n",
        "\n",
        "# --------- 4) Split e DataLoader ----------\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "X_tr, X_te, y_tr, y_te = train_test_split(X_all, y_all, test_size=0.2, stratify=y_all, random_state=0)\n",
        "X_tr, X_va, y_tr, y_va = train_test_split(X_tr, y_tr, test_size=0.25, stratify=y_tr, random_state=0)  # 60/20/20\n",
        "\n",
        "BATCH = 64\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_tr), torch.tensor(y_tr)), batch_size=BATCH, shuffle=True,  pin_memory=True)\n",
        "val_loader   = DataLoader(TensorDataset(torch.tensor(X_va), torch.tensor(y_va)), batch_size=BATCH, shuffle=False, pin_memory=True)\n",
        "test_loader  = DataLoader(TensorDataset(torch.tensor(X_te), torch.tensor(y_te)), batch_size=BATCH, shuffle=False, pin_memory=True)\n",
        "\n",
        "# --------- 5) Modello EEGPT con i TUOI 8 canali ----------\n",
        "ckpt_path = Path(\"checkpoint/eegpt_mcae_58chs_4s_large4E.ckpt\")  # metti qui il ckpt se ce l'hai\n",
        "use_pretrained = ckpt_path.exists()\n",
        "\n",
        "model = EEGPTClassifier(\n",
        "    num_classes=N_CLASSES,\n",
        "    ckpt_path=str(ckpt_path) if use_pretrained else None,\n",
        "    channels=channels,        # <<<<<< 8 canali: il resto è \"mascherato\"\n",
        "    sample_rate=FS_TGT,       # 256 Hz (dopo il resample)\n",
        "    input_time_length=int(WIN_SEC)\n",
        ").to(device)\n",
        "print(\"EEGPT init OK. Pretrained:\", use_pretrained)\n",
        "\n",
        "# --------- 6) Linear probing (congela il backbone) ----------\n",
        "for p in model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# EEGPTClassifier in genere restituisce direttamente i logits\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "def forward_logits(xb):\n",
        "    xb = xb.to(device).float()  # (B, 8, 1024)\n",
        "    out = model(xb)\n",
        "    if isinstance(out, dict) and \"logits\" in out:\n",
        "        out = out[\"logits\"]\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_loader(loader):\n",
        "    model.eval()\n",
        "    yp, yt = [], []\n",
        "    for xb, yb in loader:\n",
        "        logits = forward_logits(xb)\n",
        "        yp.append(logits.argmax(1).cpu().numpy())\n",
        "        yt.append(yb.numpy())\n",
        "    yp = np.concatenate(yp); yt = np.concatenate(yt)\n",
        "    return float(accuracy_score(yt, yp)), confusion_matrix(yt, yp, labels=list(range(N_CLASSES)))\n",
        "\n",
        "EPOCHS, PATIENCE = 5, 3\n",
        "best_val, noimp = -1.0, 0\n",
        "print(\"\\n=== Linear Probing (backbone congelato) ===\")\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    run, seen = 0.0, 0\n",
        "    pbar = tqdm(train_loader, desc=f\"[LP] {ep}/{EPOCHS}\")\n",
        "    for xb, yb in pbar:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        logits = forward_logits(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        bs = xb.size(0); run += loss.item()*bs; seen += bs\n",
        "        pbar.set_postfix(loss=run/max(1,seen))\n",
        "    va_acc, _ = eval_loader(val_loader)\n",
        "    print(f\"Val ACC: {va_acc:.3f}\")\n",
        "    if va_acc > best_val: best_val, noimp = va_acc, 0\n",
        "    else:\n",
        "        noimp += 1\n",
        "        if noimp >= PATIENCE:\n",
        "            print(\"Early stop.\")\n",
        "            break\n",
        "\n",
        "# --------- 7) Test ----------\n",
        "te_acc, cm = eval_loader(test_loader)\n",
        "print(\"\\n=== TEST ===\")\n",
        "print(\"Test ACC:\", te_acc)\n",
        "print(\"Confusion Matrix:\\n\", cm)\n",
        "print(classification_report(\n",
        "    np.concatenate([yb.numpy() for _, yb in test_loader]),\n",
        "    np.concatenate([forward_logits(xb).argmax(1).cpu().numpy() for xb, _ in test_loader]),\n",
        "    digits=3\n",
        "))\n",
        "\n"
      ]
    }
  ]
}